I was given the Netflix Data set to investigate. 
The first thing I did was get myself acquainted with the dataset. 
First, I uploaded the data set into an R notebook. I wanted to find out how many rows (5,850), and columns (15), I looked at the column types (char, int, etc.). 
Next, I opened up a Python notebook and continued to explore the data, looking at how many null values were present, the data types, etc. Then I created a correlation matrix to be able to have a cursory perusal of any correlations between factors. 
Nothing jumped out at me as very strongly correlated so I began to explore some more using tests. 
The first thing I was curious about was if a show’s rating had a correlation to its popularity score (IMDB in this case) so I asked this question: Is there a difference in IMDBscore among the age certifications of TV-MA(mature audience), TV-Y(chid audience), and TV-G(general audience)?
To begin I pulled out only shows from the data set that had those ratings and chose to run a 1 way, between-subject ANOVA (analysis of variance) on the three ratings. I tested for all necessary assumptions and upon computing post hocs I found that, there was a significant difference in score between G rating and MA rating and between Y and MA rating. There was not a significant difference between G and Y ratings. 
Next, I examined the means so that I could draw some conclusions. What I found was that television with a rating of ‘MA’ had significantly higher IMDB scores than television that was made for children and a general audience (Y and G)! 

Next, I wanted to look closer at the popularity ratings themselves and for this data set, we had two: IMDB and TMDB. I wanted to answer the question: How are the TMDB and IMDB scores related? I created a test for linearity that passed, which means that there is a linear relationship between the two ratings. Once all of the assumptions were met, I was able to create a prediction model that could predict a show or movie's IMDB score based on a given TMDB score. 

Those proved to be interesting insights into the data, but as I began to work with our group we decided we wanted to put an emphasis on how shows and movies have changed through the decades. 
So, right off the bat, I decided to run a stepwise regression to see what the strongest correlating factors were given for ‘release year’. The  Strongest correlations ended up being runtime, age_certification, imdb/tmbd_scores, and the number of seasons. After discussing with the group we decided to examine the following two questions:
Have movie and show times changed through the decades? If so, how? And, 
Have IMDB ratings changed for movies and shows over the decades? If, so how?

The first thing that I did was to break up the release year into 3 separate decades: 1990-1999, 2000-2010, and 2010-2020. 
I also broke up the data into separate categories of ‘shows’ and ‘movies’ so we could examine both individually. 
In order to answer the first question, how have running times changed through the decades, I needed to first explore the means of each and see how they compared to one another. However, after looking at the sample size present in each of the separate decades I saw that there was quite a large gap, so a traditional ANOVA probably would not be the best test to run in this situation. So, I did some research and learned that a test known as ‘Kruskalll-Wallis’ will test for variance when the data is non-parametric, and so that’s the test that I employed. 
The test for both shows and movies came back with the result that there was a significant difference between run times over the decades! So, I went back to look at the averages and found the following: Show times were much shorter in the 90s and the movies were quite a bit shorter in the 10s! - Show times increased over the years while Movie times decreased over the years. 
Running the same tests on IMDB scores over the decades showed significant variance as well. I graphed the changes using Tableau for easy visualization. 

Finally, the group wanted to examine How IMDB scores compared among the various streaming services of Hulu, Netflix, and HBO. To start that process I went back to Python to import all three data sets and then got to work coding so that I could combine them into one mega set. Once that was done I began to explore this new data set in a similar way I explored my original Netflix data set. 
Then, I calculated the IMDB score mean for Movies and the mean for SHOWS of this data set. I wanted to see if there was a statistical difference between the average popularity rating for Netflix compared to the combined data set. Would Netflix scores be lower, higher or the same? I ran a T-test on movies first and found that there was a significant difference, Movies in my Netflix data set had a lower popularity score than the overall data set. For shows, the test revealed that there was no difference between the sample and the large sample, and a look at the means showed almost identical IMDB scores for Netflix and the combined group. 
Overall I can say that Netflix is either on par or slightly below popularity rating compared to the other groups. 
